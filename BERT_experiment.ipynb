{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你', '好', '，', '我', '的', '名', '字', '是', '吳', '曉', '光']\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "from transformers import BertTokenizer,BertModel\n",
    "BERT_path_root = \"./data/chinese-roberta-wwm-ext-large\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_path_root)\n",
    "print(tokenizer.tokenize(\"你好，我的名字是吳曉光\"))\n",
    "model = BertModel.from_pretrained(BERT_path_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1203, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>section</th>\n",
       "      <th>subsection</th>\n",
       "      <th>group</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144</td>\n",
       "      <td>[臺灣史, 現代國家的形塑, 日治時期到戰後]</td>\n",
       "      <td>[台積電, 、, 聯電, 等, 知名, 半導體, 業者, 進駐, 新竹, 科學, 國區, ，...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[1, 4, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>151</td>\n",
       "      <td>[臺灣史, 現代國家的形塑, 日治時期到戰後]</td>\n",
       "      <td>[文, 中, 提及, 「, 環境, 的, 推力, 」, 是, 指, 臺灣, 內部, 的, 何...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[1, 4, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>[臺灣史, 多元族群社會的形成, 原住民族與他者的互動]</td>\n",
       "      <td>[請, 閱讀完, 下列, 文章, 後, 回答, 以下, 問題, ：, 「, 平埔族群, 的,...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1, 3, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1235</td>\n",
       "      <td>[東亞史, 現代化的歷程, 東亞國家的現代化歷程]</td>\n",
       "      <td>[下, 圖, 顯示, 一, 群, 日本, 少年, 踏, 在, 亞洲, 地圖, 上, ，, 圖...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[2, 7, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1261</td>\n",
       "      <td>[東亞史, 現代化的歷程, 共產主義在東亞的發展]</td>\n",
       "      <td>[試, 按照, 發生, 的, 先後, 順序, 排列, 此, 四, 段, 資料, ，, 下列,...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[11]</td>\n",
       "      <td>[2, 7, 18]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                         title  \\\n",
       "0   144       [臺灣史, 現代國家的形塑, 日治時期到戰後]   \n",
       "1   151       [臺灣史, 現代國家的形塑, 日治時期到戰後]   \n",
       "2    28  [臺灣史, 多元族群社會的形成, 原住民族與他者的互動]   \n",
       "3  1235     [東亞史, 現代化的歷程, 東亞國家的現代化歷程]   \n",
       "4  1261     [東亞史, 現代化的歷程, 共產主義在東亞的發展]   \n",
       "\n",
       "                                            abstract section subsection group  \\\n",
       "0  [台積電, 、, 聯電, 等, 知名, 半導體, 業者, 進駐, 新竹, 科學, 國區, ，...     [1]        [2]   [3]   \n",
       "1  [文, 中, 提及, 「, 環境, 的, 推力, 」, 是, 指, 臺灣, 內部, 的, 何...     [1]        [2]   [3]   \n",
       "2  [請, 閱讀完, 下列, 文章, 後, 回答, 以下, 問題, ：, 「, 平埔族群, 的,...     [1]        [1]   [1]   \n",
       "3  [下, 圖, 顯示, 一, 群, 日本, 少年, 踏, 在, 亞洲, 地圖, 上, ，, 圖...     [2]        [5]   [8]   \n",
       "4  [試, 按照, 發生, 的, 先後, 順序, 排列, 此, 四, 段, 資料, ，, 下列,...     [2]        [5]  [11]   \n",
       "\n",
       "       labels  \n",
       "0  [1, 4, 10]  \n",
       "1  [1, 4, 10]  \n",
       "2   [1, 3, 8]  \n",
       "3  [2, 7, 15]  \n",
       "4  [2, 7, 18]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data\n",
    "import pandas as pd\n",
    "train_raw_data = pd.read_json(\"./data/data_ch/HistoryTrain(3Lv_頌恩).json\",\n",
    "                        lines=True,\n",
    "                        orient='columns')\n",
    "print(train_raw_data.shape)\n",
    "train_raw_data.head()\n",
    "# tokenized = train_raw_data['abstract'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "# print(tokenized.shape)\n",
    "# tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 7)\n",
      "0    [101, 100, 510, 100, 5023, 100, 100, 100, 100,...\n",
      "1    [101, 3152, 704, 100, 519, 100, 4638, 100, 520...\n",
      "2                                 [101, 100, 511, 102]\n",
      "Name: abstract, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sample_data = pd.read_json(\"./data/data_ch/sample.json\",\n",
    "                        lines=True,\n",
    "                        orient='columns')\n",
    "print(sample_data.shape)\n",
    "sample_data.head()\n",
    "tokenized = sample_data['abstract'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>section</th>\n",
       "      <th>subsection</th>\n",
       "      <th>group</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>340</td>\n",
       "      <td>[臺灣史, 現代國家的形塑, 戰後的政治、社會運動]</td>\n",
       "      <td>[陳, 老師, 在, 上, 歷史課, 時, 對, 學生, 講解, 了, 南京, 《, 大剛報...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[1, 4, 11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>450</td>\n",
       "      <td>[東亞史, 國家與社會, 政治體制與統治制度]</td>\n",
       "      <td>[有關, 歷代, 土地, 制度, 之, 說明, ，, 下列, 何者, 敘述, 正確, ？, ...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2, 5, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1306</td>\n",
       "      <td>[東亞史, 現代化的歷程, 共產主義在東亞的發展]</td>\n",
       "      <td>[1978年, 12月, ，, 中共, 十一, 屆, 三中全會, 召開, ，, 這, 次, ...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[11]</td>\n",
       "      <td>[2, 7, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165</td>\n",
       "      <td>[臺灣史, 現代國家的形塑, 日治時期到戰後]</td>\n",
       "      <td>[以下, 為, 幾, 個, 同學, 對, 日本, 治, 臺, 的, 敘述, ，, 請問, 哪...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[1, 4, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>940</td>\n",
       "      <td>[東亞史, 現代化的歷程, 東亞國家的現代化歷程]</td>\n",
       "      <td>[二, 戰, 結束, 之後, ，, 東南亞, 各, 國, 紛紛, 脫離, 殖民, 獨立, 建...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[2, 7, 15]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                       title  \\\n",
       "0   340  [臺灣史, 現代國家的形塑, 戰後的政治、社會運動]   \n",
       "1   450     [東亞史, 國家與社會, 政治體制與統治制度]   \n",
       "2  1306   [東亞史, 現代化的歷程, 共產主義在東亞的發展]   \n",
       "3   165     [臺灣史, 現代國家的形塑, 日治時期到戰後]   \n",
       "4   940   [東亞史, 現代化的歷程, 東亞國家的現代化歷程]   \n",
       "\n",
       "                                            abstract section subsection group  \\\n",
       "0  [陳, 老師, 在, 上, 歷史課, 時, 對, 學生, 講解, 了, 南京, 《, 大剛報...     [1]        [2]   [4]   \n",
       "1  [有關, 歷代, 土地, 制度, 之, 說明, ，, 下列, 何者, 敘述, 正確, ？, ...     [2]        [3]   [5]   \n",
       "2  [1978年, 12月, ，, 中共, 十一, 屆, 三中全會, 召開, ，, 這, 次, ...     [2]        [5]  [11]   \n",
       "3  [以下, 為, 幾, 個, 同學, 對, 日本, 治, 臺, 的, 敘述, ，, 請問, 哪...     [1]        [2]   [3]   \n",
       "4  [二, 戰, 結束, 之後, ，, 東南亞, 各, 國, 紛紛, 脫離, 殖民, 獨立, 建...     [2]        [5]   [8]   \n",
       "\n",
       "       labels  \n",
       "0  [1, 4, 11]  \n",
       "1  [2, 5, 12]  \n",
       "2  [2, 7, 18]  \n",
       "3  [1, 4, 10]  \n",
       "4  [2, 7, 15]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Validation_raw_data = pd.read_json(\"./data/data_ch/HistoryValidation(3Lv_頌恩).json\",\n",
    "                        lines=True,\n",
    "                        orient='columns')\n",
    "print(Validation_raw_data.shape)\n",
    "Validation_raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(298, 7)\n"
     ]
    }
   ],
   "source": [
    "Test_raw_data = pd.read_json(\"./data/data_ch/HistoryTest(3Lv_頌恩).json\",\n",
    "                        lines=True,\n",
    "                        orient='columns')\n",
    "print(Test_raw_data.shape)\n",
    "# Test_raw_data['abstract'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_raw_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-3a9601e646f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_raw_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_raw_data' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized = train_raw_data['title'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def bert_encode(data,maximum_length) :\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            data[i],\n",
    "            add_special_tokens=True,\n",
    "            max_length=maximum_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids,train_attention_masks = bert_encode(train_df['review'][:5],1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertModel' object has no attribute 'wv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-45bec5318dc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword2idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mword2idx_sorted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\W2VtoBERT\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m-> 1178\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'wv'"
     ]
    }
   ],
   "source": [
    "word2idx = dict([(k, v.index) for k, v in model.wv.vocab.items()])\n",
    "word2idx_sorted = [(k, word2idx[k]) for k in sorted(word2idx, key=word2idx.get, reverse=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_path_root = \"./data/chinese_roberta_wwm_large_ext_pytorch\"\n",
    "bert_config_file_path = './data/chinese_roberta_wwm_large_ext_pytorch/bert_config.json'\n",
    "# bert_config = modeling.BertConfig.from_json_file(bert_config_file)\n",
    "init_checkpoint_path = os.path.join(BERT_path_root, 'bert_model.ckpt')\n",
    "bert_vocab_file_path = os.path.join(BERT_path_root, 'vocab.txt')\n",
    "print(bert_config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 647/647 [00:00<00:00, 649kB/s]\n",
      "Downloading: 100%|██████████| 110k/110k [00:00<00:00, 166kB/s]  \n",
      "Downloading: 100%|██████████| 269k/269k [00:00<00:00, 316kB/s]  \n",
      "Downloading: 100%|██████████| 2.00/2.00 [00:00<00:00, 2.00kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 112kB/s]\n",
      "Downloading: 100%|██████████| 19.0/19.0 [00:00<00:00, 19.0kB/s]\n",
      "Downloading: 100%|██████████| 412M/412M [00:04<00:00, 82.4MB/s] \n",
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-bert-wwm-ext\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"hfl/chinese-bert-wwm-ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 690/690 [00:00<00:00, 690kB/s]\n",
      "Downloading: 100%|██████████| 110k/110k [00:00<00:00, 161kB/s]  \n",
      "Downloading: 100%|██████████| 269k/269k [00:00<00:00, 305kB/s]  \n",
      "Downloading: 100%|██████████| 2.00/2.00 [00:00<00:00, 1.95kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 110kB/s]\n",
      "Downloading: 100%|██████████| 19.0/19.0 [00:00<00:00, 19.0kB/s]\n",
      "Downloading: 100%|██████████| 1.31G/1.31G [00:45<00:00, 28.9MB/s]   \n",
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b988e9cef0b27b3784eb128b275c3533028856e0c9a2ef9485d8a89547c3c962"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('HARNNrec': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
